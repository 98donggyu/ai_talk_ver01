# app/vector_db.py

import uuid
import time
import asyncio
from pinecone import Pinecone, ServerlessSpec

from . import config
from .ai_services import get_embedding, get_ai_chat_completion

pc = Pinecone(api_key=config.PINECONE_API_KEY)
if config.PINECONE_INDEX_NAME not in pc.list_indexes().names():
    pc.create_index(
        name=config.PINECONE_INDEX_NAME, dimension=1536, metric="cosine", 
        spec=ServerlessSpec(cloud="aws", region="us-east-1")
    )
index = pc.Index(config.PINECONE_INDEX_NAME)
print(f"âœ… Pinecone '{config.PINECONE_INDEX_NAME}' ì¸ë±ìŠ¤ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")


async def create_memory_for_pinecone(user_id: str, current_session_log: list):
    print(f"ğŸ§  [{user_id}] ë‹˜ì˜ ì„¸ì…˜ ê¸°ì–µ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    if not current_session_log: return

    is_short_conversation = len(current_session_log) < 4
    memory_text, memory_type = "", ""

    if is_short_conversation:
        print("-> ì§§ì€ ëŒ€í™”ë¡œ íŒë‹¨, ëŒ€í™” ì›ë¬¸ì„ 'utterance' íƒ€ì…ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
        memory_text = "\n".join(current_session_log)
        memory_type = 'utterance'
    else:
        print("-> ê¸´ ëŒ€í™”ë¡œ íŒë‹¨, í•µì‹¬ ìš”ì•½ì„ 'summary' íƒ€ì…ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        conversation_history = "\n".join(current_session_log)
        summary_prompt = "ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì—ì„œ ì‚¬ìš©ìì˜ ì£¼ìš” ê´€ì‹¬ì‚¬, ê°ì •, ì¤‘ìš”í•œ ì •ë³´ ë“±ì„ 1~2 ë¬¸ì¥ì˜ ê°„ê²°í•œ ê¸°ì–µìœ¼ë¡œ ìƒì„±í•´ì¤˜. ê·œì¹™: ì§€ëª…, ì¸ëª… ë“± ëª¨ë“  ê³ ìœ ëª…ì‚¬ëŠ” ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì•¼ í•´.\n\n--- ëŒ€í™” ë‚´ìš© ---\n{conversation_history}\n-----------------\n\ní•µì‹¬ ê¸°ì–µ:"
        memory_text = await get_ai_chat_completion(summary_prompt.format(conversation_history=conversation_history), max_tokens=200, temperature=0.3)
        memory_type = 'summary'

    print(f"ğŸ“ ìƒì„±ëœ ê¸°ì–µ (íƒ€ì…: {memory_type}): {memory_text}")
    embedding = await get_embedding(memory_text)
    
    vector_to_upsert = {
        'id': str(uuid.uuid4()), 'values': embedding,
        'metadata': {'user_id': user_id, 'text': memory_text, 'timestamp': int(time.time()), 'memory_type': memory_type}
    }
    await asyncio.to_thread(index.upsert, vectors=[vector_to_upsert])
    print(f"âœ… [{user_id}] ë‹˜ì˜ ìƒˆë¡œìš´ ì„¸ì…˜ ê¸°ì–µì´ Pineconeì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")


async def search_memories(user_id: str, query_message: str, top_k=5):
    query_embedding = await get_embedding(query_message)
    results = await asyncio.to_thread(index.query, vector=query_embedding, top_k=top_k, filter={'user_id': user_id}, include_metadata=True)
    
    now, ranked_memories = int(time.time()), []
    for match in results['matches']:
        similarity_score = match['score']; metadata = match.get('metadata', {}); timestamp = metadata.get('timestamp', now)
        time_decay_factor = 30 * 24 * 60 * 60
        recency_score = max(0, (timestamp - (now - time_decay_factor)) / time_decay_factor)
        final_score = (similarity_score * 0.7) + (recency_score * 0.3)
        ranked_memories.append({'text': metadata.get('text', ''), 'score': final_score})
        
    ranked_memories.sort(key=lambda x: x['score'], reverse=True)
    top_memories = [item['text'] for item in ranked_memories[:3]]
    print(f"ğŸ” [{user_id}] ë‹˜ì˜ ê³¼ê±° í•µì‹¬ ê¸°ì–µ {len(top_memories)}ê°œë¥¼ ì¬ì •ë ¬í•˜ì—¬ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
    return "\n".join(top_memories)