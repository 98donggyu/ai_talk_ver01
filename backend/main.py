import os
import uuid
import json
import base64
import tempfile
import asyncio
import time
from datetime import datetime, timedelta
from dotenv import load_dotenv

import openai
from fastapi import FastAPI, WebSocket, WebSocketDisconnect, Query
from fastapi.middleware.cors import CORSMiddleware
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

from pinecone import Pinecone, ServerlessSpec

# --- 1. ì´ˆê¸° ì„¤ì • (í™˜ê²½ ë³€ìˆ˜, DB, Pinecone, FastAPI ì•±) ---

load_dotenv()

# prompts.json íŒŒì¼ì„ ì½ì–´ì„œ íŒŒì´ì¬ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
try:
    with open('prompts.json', 'r', encoding='utf-8') as f:
        PROMPTS_CONFIG = json.load(f)['main_chat_prompt']
    print("âœ… prompts.json íŒŒì¼ì„ ì„±ê³µì ìœ¼ë¡œ ë¶ˆëŸ¬ì™”ìŠµë‹ˆë‹¤.")
except FileNotFoundError:
    print("âŒ prompts.json íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. main.pyì™€ ê°™ì€ ìœ„ì¹˜ì— ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
    exit()


DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_HOST = os.getenv("DB_HOST")
DB_NAME = "ai_talk_db"

def init_db():
    """ì„œë²„ ì‹œì‘ ì‹œ ë°ì´í„°ë² ì´ìŠ¤ì™€ í…Œì´ë¸”ì„ í™•ì¸í•˜ê³  ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        server_engine = create_engine(f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}?charset=utf8mb4")
        with server_engine.connect() as connection:
            connection.execute(text(f"CREATE DATABASE IF NOT EXISTS {DB_NAME} CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci"))
        
        db_engine = create_engine(f"mysql+pymysql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}/{DB_NAME}?charset=utf8mb4")
        with db_engine.connect() as connection:
            connection.execute(text("""
            CREATE TABLE IF NOT EXISTS conversations (
                id INT AUTO_INCREMENT PRIMARY KEY, user_id VARCHAR(255) NOT NULL,
                user_message TEXT NOT NULL, ai_message TEXT NOT NULL,
                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
            );"""))
            connection.execute(text("""
            CREATE TABLE IF NOT EXISTS summaries (
                id INT AUTO_INCREMENT PRIMARY KEY, user_id VARCHAR(255) NOT NULL,
                summary_text TEXT NOT NULL, created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                INDEX (user_id)
            );"""))
        print("âœ… MySQL ë°ì´í„°ë² ì´ìŠ¤ ë° í…Œì´ë¸”ì´ ì„±ê³µì ìœ¼ë¡œ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return db_engine
    except Exception as e:
        print(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì • ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

engine = init_db()
if engine is None:
    exit("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨ë¡œ ì„œë²„ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)
app = FastAPI()
app.add_middleware(CORSMiddleware, allow_origins=["*"], allow_credentials=True, allow_methods=["*"], allow_headers=["*"])
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
pc = Pinecone(api_key=os.getenv("PINECONE_API_KEY"))

index_name = "long-term-memory"
if index_name not in pc.list_indexes().names():
    pc.create_index(name=index_name, dimension=1536, metric="cosine", spec=ServerlessSpec(cloud="aws", region="us-east-1"))
index = pc.Index(index_name)
print(f"âœ… Pinecone '{index_name}' ì¸ë±ìŠ¤ì— ì„±ê³µì ìœ¼ë¡œ ì—°ê²°ë˜ì—ˆìŠµë‹ˆë‹¤.")


class ConnectionManager:
    def __init__(self):
        self.active_connections: dict[str, WebSocket] = {}
    async def connect(self, websocket: WebSocket, user_id: str):
        await websocket.accept()
        self.active_connections[user_id] = websocket
    def disconnect(self, user_id: str):
        if user_id in self.active_connections:
            del self.active_connections[user_id]
    async def send_json(self, data: dict, user_id: str):
        if user_id in self.active_connections:
            await self.active_connections[user_id].send_text(json.dumps(data))

manager = ConnectionManager()
session_conversations = {}


# --- 2. ë°ì´í„° ì²˜ë¦¬ í•µì‹¬ í•¨ìˆ˜ë“¤ ---

async def get_embedding(text):
    response = await asyncio.to_thread(client.embeddings.create, input=text, model="text-embedding-3-small")
    return response.data[0].embedding

async def save_conversation_to_mysql(user_id: str, user_message: str, ai_message: str):
    db = SessionLocal()
    try:
        query = text("INSERT INTO conversations (user_id, user_message, ai_message) VALUES (:user_id, :user_message, :ai_message)")
        db.execute(query, {"user_id": user_id, "user_message": user_message, "ai_message": ai_message})
        db.commit()
    finally:
        db.close()

async def create_memory_for_pinecone(user_id: str, current_session_log: list):
    """í˜„ì¬ ì„¸ì…˜ì˜ ëŒ€í™” ê¸°ë¡ìœ¼ë¡œ Pineconeì— ì €ì¥í•  ê¸°ì–µ(ìš”ì•½ ë˜ëŠ” ì›ë¬¸)ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"ğŸ§  [{user_id}] ë‹˜ì˜ ì„¸ì…˜ ê¸°ì–µ ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    if not current_session_log:
        print("-> ì´ë²ˆ ì„¸ì…˜ì— ëŒ€í™” ë‚´ìš©ì´ ì—†ì–´, ê¸°ì–µì„ ìƒì„±í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return

    is_short_conversation = len(current_session_log) < 4
    memory_text, memory_type = "", ""

    if is_short_conversation:
        print("-> ì§§ì€ ëŒ€í™”ë¡œ íŒë‹¨, ëŒ€í™” ì›ë¬¸ì„ 'utterance' íƒ€ì…ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.")
        memory_text = "\n".join(current_session_log)
        memory_type = 'utterance'
    else:
        print("-> ê¸´ ëŒ€í™”ë¡œ íŒë‹¨, í•µì‹¬ ìš”ì•½ì„ 'summary' íƒ€ì…ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
        conversation_history = "\n".join(current_session_log)
        summary_prompt = "ë‹¤ìŒ ëŒ€í™” ë‚´ìš©ì—ì„œ ì‚¬ìš©ìì˜ ì£¼ìš” ê´€ì‹¬ì‚¬, ê°ì •, ì¤‘ìš”í•œ ì •ë³´ ë“±ì„ 1~2 ë¬¸ì¥ì˜ ê°„ê²°í•œ ê¸°ì–µìœ¼ë¡œ ìƒì„±í•´ì¤˜. ê·œì¹™: ì§€ëª…, ì¸ëª… ë“± ëª¨ë“  ê³ ìœ ëª…ì‚¬ëŠ” ë°˜ë“œì‹œ í¬í•¨ì‹œì¼œì•¼ í•´.\n\n--- ëŒ€í™” ë‚´ìš© ---\n{conversation_history}\n-----------------\n\ní•µì‹¬ ê¸°ì–µ:"
        summary_response = await asyncio.to_thread(
            client.chat.completions.create,
            model="gpt-4o",
            messages=[{"role": "user", "content": summary_prompt.format(conversation_history=conversation_history)}],
            max_tokens=200,
            temperature=0.3
        )
        memory_text = summary_response.choices[0].message.content
        memory_type = 'summary'

    print(f"ğŸ“ ìƒì„±ëœ ê¸°ì–µ (íƒ€ì…: {memory_type}): {memory_text}")
    embedding = await get_embedding(memory_text)
    
    vector_to_upsert = {
        'id': str(uuid.uuid4()),
        'values': embedding,
        'metadata': {
            'user_id': user_id,
            'text': memory_text,
            'timestamp': int(time.time()),
            'memory_type': memory_type
        }
    }
    
    await asyncio.to_thread(index.upsert, vectors=[vector_to_upsert])
    print(f"âœ… [{user_id}] ë‹˜ì˜ ìƒˆë¡œìš´ ì„¸ì…˜ ê¸°ì–µì´ Pineconeì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")

async def create_hourly_summary_report(user_id: str):
    """1ì‹œê°„ ì£¼ê¸°ë¡œ ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©ì„ ì¢…í•©í•˜ì—¬ 'ì‹œê°„ë³„ ë¦¬í¬íŠ¸'ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    print(f"ğŸ—“ï¸ [{user_id}] ë‹˜ì˜ ì‹œê°„ë³„ ë¦¬í¬íŠ¸ ìƒì„± ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.")
    db = SessionLocal()
    try:
        last_summary_query = text("SELECT created_at FROM summaries WHERE user_id = :user_id ORDER BY created_at DESC LIMIT 1")
        last_summary_time = db.execute(last_summary_query, {"user_id": user_id}).scalar_one_or_none()

        one_hour_ago = datetime.now() - timedelta(hours=1)
        if last_summary_time and last_summary_time > one_hour_ago:
            print("-> ë§ˆì§€ë§‰ ë¦¬í¬íŠ¸ ìƒì„± í›„ 1ì‹œê°„ì´ ì§€ë‚˜ì§€ ì•Šì•„, ìƒì„±ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return

        start_time = last_summary_time or datetime.min
        new_conv_query = text("SELECT user_message, ai_message FROM conversations WHERE user_id = :user_id AND created_at > :start_time")
        new_conversations = db.execute(new_conv_query, {"user_id": user_id, "start_time": start_time}).fetchall()

        if not new_conversations:
            print("-> ë¦¬í¬íŠ¸ì— ì¶”ê°€í•  ìƒˆë¡œìš´ ëŒ€í™” ë‚´ìš©ì´ ì—†ìŠµë‹ˆë‹¤.")
            return

        print(f"-> ìƒˆë¡œìš´ ëŒ€í™” {len(new_conversations)}ê±´ì„ ë°”íƒ•ìœ¼ë¡œ ì‹œê°„ë³„ ë¦¬í¬íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.")
        conversation_history = "\n".join([f"ì‚¬ìš©ì: {row[0]}\nAI: {row[1]}" for row in new_conversations])
        report_prompt = f"ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ìµœê·¼ ëŒ€í™” ë‚´ìš©ì…ë‹ˆë‹¤. ì´ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ìƒíƒœì™€ ì£¼ìš” ëŒ€í™” ë‚´ìš©ì„ ìš”ì•½í•˜ëŠ” 'ì‹œê°„ë³„ ë¦¬í¬íŠ¸'ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”.\n\n--- ëŒ€í™” ë‚´ìš© ---\n{conversation_history}\n-----------------\n\nì‹œê°„ë³„ ë¦¬í¬íŠ¸:"
        
        report_response = await asyncio.to_thread(client.chat.completions.create, model="gpt-4o", messages=[{"role": "user", "content": report_prompt}])
        report_text = report_response.choices[0].message.content
        
        summary_save_query = text("INSERT INTO summaries (user_id, summary_text) VALUES (:user_id, :summary_text)")
        db.execute(summary_save_query, {"user_id": user_id, "summary_text": report_text})
        db.commit()
        print(f"âœ… [{user_id}] ë‹˜ì˜ ì‹œê°„ë³„ ë¦¬í¬íŠ¸ê°€ MySQL summaries í…Œì´ë¸”ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    finally:
        db.close()

async def search_memories(user_id: str, query_message: str, top_k=5):
    """Pineconeì—ì„œ ê´€ë ¨ ê¸°ì–µì„ ê²€ìƒ‰í•˜ê³ , ìµœì‹  ê¸°ì–µì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•´ ì¬ì •ë ¬í•©ë‹ˆë‹¤."""
    query_embedding = await get_embedding(query_message)
    results = await asyncio.to_thread(index.query, vector=query_embedding, top_k=top_k, filter={'user_id': user_id}, include_metadata=True)
    
    now = int(time.time())
    ranked_memories = []
    
    for match in results['matches']:
        similarity_score = match['score']
        metadata = match.get('metadata', {})
        timestamp = metadata.get('timestamp', now)
        time_decay_factor = 30 * 24 * 60 * 60
        recency_score = max(0, (timestamp - (now - time_decay_factor)) / time_decay_factor)
        final_score = (similarity_score * 0.7) + (recency_score * 0.3)
        ranked_memories.append({'text': metadata.get('text', ''), 'score': final_score})
        
    ranked_memories.sort(key=lambda x: x['score'], reverse=True)
    top_memories = [item['text'] for item in ranked_memories[:3]]
    
    print(f"ğŸ” [{user_id}] ë‹˜ì˜ ê³¼ê±° í•µì‹¬ ê¸°ì–µ {len(top_memories)}ê°œë¥¼ ì¬ì •ë ¬í•˜ì—¬ ê²€ìƒ‰í–ˆìŠµë‹ˆë‹¤.")
    return "\n".join(top_memories)

async def process_audio_and_get_response(user_id: str, audio_base64: str):
    """ì˜¤ë””ì˜¤ ì²˜ë¦¬, ê¸°ì–µ ê²€ìƒ‰, AI ì‘ë‹µ ìƒì„±, DB ì €ì¥ì„ ì´ê´„í•˜ë©° JSON í”„ë¡¬í”„íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤."""
    audio_data = base64.b64decode(audio_base64)
    with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as temp_audio:
        temp_audio.write(audio_data); temp_audio_path = temp_audio.name
    try:
        with open(temp_audio_path, "rb") as audio_file:
            transcript_response = await asyncio.to_thread(client.audio.transcriptions.create, model="whisper-1", file=audio_file, language="ko")
        user_message = transcript_response.text
        if not user_message.strip() or "ì‹œì²­í•´ì£¼ì…”ì„œ ê°ì‚¬í•©ë‹ˆë‹¤" in user_message:
            return None, "ìŒ, ì˜ ì•Œì•„ë“£ì§€ ëª»í–ˆì–´ìš”. í˜¹ì‹œ ë‹¤ì‹œ í•œë²ˆ ë§ì”€í•´ì£¼ì‹œê² ì–´ìš”?"
        
        relevant_memories = await search_memories(user_id, user_message)
        
        # prompts.json ë‚´ìš©ì„ ì¡°í•©í•˜ì—¬ ìµœì¢… í”„ë¡¬í”„íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ìƒì„±
        system_message = "\n".join(PROMPTS_CONFIG['system_message_base'])
        examples_text = "\n\n".join([f"ìƒí™©: {ex['situation']}\nì‚¬ìš©ì ì…ë ¥: {ex['user_input']}\nAI ì‘ë‹µ: {ex['ai_response']}\n" for ex in PROMPTS_CONFIG['examples']])
        
        final_prompt = f"""
# í˜ë¥´ì†Œë‚˜
{system_message}

# í•µì‹¬ ëŒ€í™” ê·œì¹™
{"\n".join(PROMPTS_CONFIG['core_conversation_rules'])}

# ì‘ë‹µ ê°€ì´ë“œë¼ì¸
{"\n".join(PROMPTS_CONFIG['guidelines_and_reactions'])}

# ì ˆëŒ€ ê¸ˆì§€ì‚¬í•­
{"\n".join(PROMPTS_CONFIG['strict_prohibitions'])}

# ì„±ê³µì ì¸ ëŒ€í™” ì˜ˆì‹œ
{examples_text}

---
ì´ì œ ì‹¤ì œ ëŒ€í™”ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤.

--- ê³¼ê±° ëŒ€í™” í•µì‹¬ ê¸°ì–µ ---
{relevant_memories if relevant_memories else "ì´ì „ ëŒ€í™” ê¸°ë¡ì´ ì—†ìŠµë‹ˆë‹¤."}
--------------------

í˜„ì¬ ì‚¬ìš©ì ë©”ì‹œì§€: "{user_message}"
AI ë‹µë³€:
"""
        
        chat_response = await asyncio.to_thread(
            client.chat.completions.create,
            model="gpt-4o",
            messages=[
                {"role": "system", "content": "ë‹¹ì‹ ì€ ì£¼ì–´ì§„ ê·œì¹™ê³¼ í˜ë¥´ì†Œë‚˜ë¥¼ ì™„ë²½í•˜ê²Œ ë”°ë¥´ëŠ” AI ì–´ì‹œìŠ¤í„´íŠ¸ì…ë‹ˆë‹¤."},
                {"role": "user", "content": final_prompt}
            ],
            max_tokens=150,
            temperature=0.7
        )
        ai_response = chat_response.choices[0].message.content
        
        await save_conversation_to_mysql(user_id, user_message, ai_response)
        
        if user_id in session_conversations:
            session_conversations[user_id].append(f"ì‚¬ìš©ì: {user_message}")
            session_conversations[user_id].append(f"AI: {ai_response}")
            
        return user_message, ai_response
    finally:
        os.unlink(temp_audio_path)


# --- 3. ì›¹ì†Œì¼“ ì—”ë“œí¬ì¸íŠ¸ ë° ì„¸ì…˜ ê´€ë¦¬ ---

@app.websocket("/ws/chat")
async def websocket_endpoint(websocket: WebSocket, user_id: str = Query(...)):
    """ì›¹ì†Œì¼“ ì—°ê²° ë° ì„¸ì…˜ì˜ ì‹œì‘ê³¼ ëì„ ê´€ë¦¬í•©ë‹ˆë‹¤."""
    await manager.connect(websocket, user_id)
    session_conversations[user_id] = []
    print(f"í´ë¼ì´ì–¸íŠ¸ [{user_id}] ì—°ê²°ë¨. ìƒˆ ì„¸ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    
    start_question = "ì•ˆë…•í•˜ì„¸ìš”! ì˜¤ëŠ˜ì€ ì–´ë–¤ í•˜ë£¨ë¥¼ ë³´ë‚´ê³  ê³„ì‹ ê°€ìš”?"
    await manager.send_json({"type": "ai_message", "content": start_question}, user_id)
    
    try:
        while True:
            data = await websocket.receive_text()
            message_data = json.loads(data)
            if message_data.get("type") == "audio_data":
                user_message, ai_response = await process_audio_and_get_response(user_id, message_data["audio"])
                if user_message:
                    await manager.send_json({"type": "user_message", "content": user_message}, user_id)
                await manager.send_json({"type": "ai_message", "content": ai_response}, user_id)
            
    except WebSocketDisconnect:
        print(f"í´ë¼ì´ì–¸íŠ¸ [{user_id}] ì—°ê²°ì´ ëŠì–´ì¡ŒìŠµë‹ˆë‹¤.")
    finally:
        # ì„¸ì…˜ ì¢…ë£Œ ì‹œ ê¸°ì–µ ìƒì„± ë° ë¦¬í¬íŠ¸ ìƒì„± ì‹œë„
        if user_id in session_conversations:
            current_session_log = session_conversations.pop(user_id)
            # ë‘ ì‘ì—…ì„ ë™ì‹œì— ë¹„ë™ê¸°ì ìœ¼ë¡œ ì‹¤í–‰
            await asyncio.gather(
                create_memory_for_pinecone(user_id, current_session_log),
                create_hourly_summary_report(user_id)
            )
            print(f"ì„¸ì…˜ ë°ì´í„° ì²˜ë¦¬ ë° ì •ë¦¬ ì™„ë£Œ: {user_id}")

        manager.disconnect(user_id)
        print(f"[{user_id}] í´ë¼ì´ì–¸íŠ¸ì™€ì˜ ëª¨ë“  ì²˜ë¦¬ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

@app.get("/")
async def root():
    return {"message": "AI Talk Backend is Running"}